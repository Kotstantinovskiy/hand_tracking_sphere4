{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = \"Hand Tracking\"\n",
    "PALM_MODEL_PATH = \"./palm_detection_without_custom_op.tflite\"\n",
    "LANDMARK_MODEL_PATH = \"./hand_landmark.tflite\"\n",
    "#LANDMARK_MODEL_PATH = \"./hand_landmark_3d.tflite\"\n",
    "ANCHORS_PATH = \"./anchors.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandTracker():\n",
    "    \n",
    "    def __init__(self, palm_model,\n",
    "                joint_model,\n",
    "                anchors_path,\n",
    "                box_enlarge=1.5,\n",
    "                box_shift=0.2):\n",
    "        \n",
    "        self.box_shift = box_shift\n",
    "        self.box_enlarge = box_enlarge\n",
    "\n",
    "        self.interp_palm = tf.lite.Interpreter(palm_model)\n",
    "        self.interp_palm.allocate_tensors()\n",
    "        self.interp_joint = tf.lite.Interpreter(joint_model)\n",
    "        self.interp_joint.allocate_tensors()\n",
    "\n",
    "        # reading the SSD anchors\n",
    "        with open(anchors_path, \"r\") as csv_f:\n",
    "            self.anchors = np.r_[\n",
    "                [x for x in csv.reader(csv_f, quoting=csv.QUOTE_NONNUMERIC)]\n",
    "            ]\n",
    "        # reading tflite model paramteres\n",
    "        output_details = self.interp_palm.get_output_details()\n",
    "        input_details = self.interp_palm.get_input_details()\n",
    "\n",
    "        self.in_idx = input_details[0]['index']\n",
    "        self.out_reg_idx = output_details[0]['index']\n",
    "        self.out_clf_idx = output_details[1]['index']\n",
    "\n",
    "        self.in_idx_joint = self.interp_joint.get_input_details()[0]['index']\n",
    "        self.out_idx_joint = self.interp_joint.get_output_details()[0]['index']\n",
    "\n",
    "        # 90Â° rotation matrix used to create the alignment trianlge\n",
    "        self.R90 = np.r_[[[0,1],[-1,0]]]\n",
    "\n",
    "        # trianlge target coordinates used to move the detected hand\n",
    "        # into the right position\n",
    "        self._target_triangle = np.float32([\n",
    "                        [128, 128],\n",
    "                        [128,   0],\n",
    "                        [  0, 128]\n",
    "                    ])\n",
    "        self._target_box = np.float32([\n",
    "                        [  0,   0, 1],\n",
    "                        [256,   0, 1],\n",
    "                        [256, 256, 1],\n",
    "                        [  0, 256, 1],\n",
    "                    ])\n",
    "\n",
    "    def _get_triangle(self, kp0, kp2, dist=1):\n",
    "        dir_v = kp2 - kp0\n",
    "        dir_v /= np.linalg.norm(dir_v)\n",
    "\n",
    "        dir_v_r = dir_v @ self.R90.T\n",
    "        return np.float32([kp2, kp2+dir_v*dist, kp2 + dir_v_r*dist])\n",
    "\n",
    "    @staticmethod\n",
    "    def _triangle_to_bbox(source):\n",
    "        # plain old vector arithmetics\n",
    "        bbox = np.c_[\n",
    "            [source[2] - source[0] + source[1]],\n",
    "            [source[1] + source[0] - source[2]],\n",
    "            [3 * source[0] - source[1] - source[2]],\n",
    "            [source[2] - source[1] + source[0]],\n",
    "        ].reshape(-1,2)\n",
    "        return bbox\n",
    "\n",
    "    @staticmethod\n",
    "    def _im_normalize(img):\n",
    "         return np.ascontiguousarray(2 * ((img / 255) - 0.5).astype('float32'))\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigm(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def _pad1(x):\n",
    "        return np.pad(x, ((0, 0), (0, 1)), constant_values=1, mode='constant')\n",
    "\n",
    "    def predict_joints(self, img_norm):\n",
    "        self.interp_joint.set_tensor(self.in_idx_joint, img_norm.reshape(1,256,256,3))\n",
    "        self.interp_joint.invoke()\n",
    "\n",
    "        joints = self.interp_joint.get_tensor(self.out_idx_joint)\n",
    "        \n",
    "        return joints.reshape(-1,2)\n",
    "        #return joints.reshape(-1, 3)\n",
    "\n",
    "    # predict hand location and landmarks\n",
    "    def detect_hand(self, img_norm):\n",
    "        self.interp_palm.set_tensor(self.in_idx, img_norm[None])\n",
    "        self.interp_palm.invoke()\n",
    "\n",
    "        out_reg = self.interp_palm.get_tensor(self.out_reg_idx)[0]\n",
    "        out_clf = self.interp_palm.get_tensor(self.out_clf_idx)[0,:,0]\n",
    "        \n",
    "        detecion_mask = self._sigm(out_clf) > 0.85\n",
    "        candidate_detect = out_reg[detecion_mask]\n",
    "        candidate_anchors = self.anchors[detecion_mask]\n",
    "\n",
    "        if candidate_detect.shape[0] == 0:\n",
    "            #print(\"No hands found\")\n",
    "            return None, None\n",
    "        \n",
    "        max_idx = np.argmax(candidate_detect[:, 3])\n",
    "        \n",
    "        dx,dy,w,h = candidate_detect[max_idx, :4]\n",
    "        center_wo_offst = candidate_anchors[max_idx,:2] * 256\n",
    "        \n",
    "        keypoints = center_wo_offst + candidate_detect[max_idx,4:].reshape(-1,2)\n",
    "        side = max(w,h) * self.box_enlarge\n",
    "        \n",
    "        source = self._get_triangle(keypoints[0], keypoints[2], side)\n",
    "        source -= (keypoints[0] - keypoints[2]) * self.box_shift\n",
    "        return source, keypoints\n",
    "\n",
    "    #normalize image to 256x256\n",
    "    def preprocess_img(self, img):\n",
    "        shape = np.r_[img.shape]\n",
    "        pad = (shape.max() - shape[:2]).astype('uint32') // 2\n",
    "        img_pad = np.pad(img, ((pad[0],pad[0]), (pad[1],pad[1]), (0,0)), mode='constant')\n",
    "        \n",
    "        img_small = cv2.resize(img_pad, (256, 256))\n",
    "        img_small = np.ascontiguousarray(img_small)\n",
    "\n",
    "        img_norm = self._im_normalize(img_small)\n",
    "        return img_pad, img_norm, pad\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img_pad, img_norm, pad = self.preprocess_img(img)\n",
    "\n",
    "        source, keypoints = self.detect_hand(img_norm)\n",
    "        if source is None:\n",
    "            return None, None\n",
    "\n",
    "        # calculating transformation from img_pad coords\n",
    "        # to img_landmark coords (cropped hand image)\n",
    "        scale = max(img.shape) / 256\n",
    "        Mtr = cv2.getAffineTransform(\n",
    "            source * scale,\n",
    "            self._target_triangle\n",
    "        )\n",
    "\n",
    "        img_landmark = cv2.warpAffine(\n",
    "            self._im_normalize(img_pad), Mtr, (256,256)\n",
    "        )\n",
    "\n",
    "        joints = self.predict_joints(img_landmark)\n",
    "\n",
    "        # adding the [0,0,1] row to make the matrix square\n",
    "        Mtr = self._pad1(Mtr.T).T\n",
    "        Mtr[2,:2] = 0\n",
    "\n",
    "        Minv = np.linalg.inv(Mtr)\n",
    "\n",
    "        # projecting keypoints back into original image coordinate space\n",
    "        kp_orig = (self._pad1(joints[:, :2]) @ Minv.T)[:,:2]\n",
    "        box_orig = (self._target_box @ Minv.T)[:,:2]\n",
    "        kp_orig -= pad[::-1]\n",
    "        box_orig -= pad[::-1]\n",
    "\n",
    "        return kp_orig, box_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "POINT_COLOR = (0, 255, 0)\n",
    "CONNECTION_COLOR = (255, 0, 0)\n",
    "THICKNESS = 2\n",
    "\n",
    "cv2.namedWindow(WINDOW)\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "if capture.isOpened():\n",
    "    hasFrame, frame = capture.read()\n",
    "else:\n",
    "    hasFrame = False\n",
    "\n",
    "connections = [\n",
    "    (0, 1), (1, 2), (2, 3), (3, 4),\n",
    "    (5, 6), (6, 7), (7, 8),\n",
    "    (9, 10), (10, 11), (11, 12),\n",
    "    (13, 14), (14, 15), (15, 16),\n",
    "    (17, 18), (18, 19), (19, 20),\n",
    "    (0, 5), (5, 9), (9, 13), (13, 17), (0, 17)\n",
    "]\n",
    "\n",
    "detector = HandTracker(\n",
    "    PALM_MODEL_PATH,\n",
    "    LANDMARK_MODEL_PATH,\n",
    "    ANCHORS_PATH,\n",
    "    box_shift=0.2,\n",
    "    box_enlarge=1.3\n",
    ")\n",
    "\n",
    "while hasFrame:\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    points, bbox = detector(image)\n",
    "    \n",
    "    if points is not None:\n",
    "        '''\n",
    "        for point in points:\n",
    "            x, y = point\n",
    "            cv2.circle(frame, (int(x), int(y)), THICKNESS*2, POINT_COLOR, THICKNESS)\n",
    "        '''\n",
    "        \n",
    "        cv2.line(frame, (int(bbox[0][0]), int(bbox[0][1])),\n",
    "                 (int(bbox[1][0]), int(bbox[1][1])), POINT_COLOR, 1)\n",
    "        cv2.line(frame, (int(bbox[1][0]), int(bbox[1][1])),\n",
    "                 (int(bbox[2][0]), int(bbox[2][1])), POINT_COLOR, 1) \n",
    "        cv2.line(frame, (int(bbox[2][0]), int(bbox[2][1])),\n",
    "                 (int(bbox[3][0]), int(bbox[3][1])), POINT_COLOR, 1) \n",
    "        cv2.line(frame, (int(bbox[3][0]), int(bbox[3][1])),\n",
    "                 (int(bbox[0][0]), int(bbox[0][1])), POINT_COLOR, 1)\n",
    "        \n",
    "        '''\n",
    "        for connection in connections:\n",
    "            x0, y0 = points[connection[0]]\n",
    "            x1, y1 = points[connection[1]]\n",
    "            cv2.line(frame, (int(x0), int(y0)), (int(x1), int(y1)), CONNECTION_COLOR, THICKNESS)\n",
    "        '''\n",
    "        \n",
    "    cv2.imshow(WINDOW, frame)\n",
    "    hasFrame, frame = capture.read()\n",
    "    key = cv2.waitKey(20)\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#        8   12  16  20\n",
    "#        |   |   |   |\n",
    "#        7   11  15  19\n",
    "#    4   |   |   |   |\n",
    "#    |   6   10  14  18\n",
    "#    3   |   |   |   |\n",
    "#    |   5---9---13--17\n",
    "#    2    \\         /\n",
    "#     \\    \\       /\n",
    "#      1    \\     /\n",
    "#       \\    \\   /\n",
    "#        ------0-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8571\n",
      "12841\n",
      "1\n",
      "4677\n"
     ]
    }
   ],
   "source": [
    "output_file = open(\"output.txt\", \"w\")\n",
    "\n",
    "detector = HandTracker(\n",
    "    PALM_MODEL_PATH,\n",
    "    LANDMARK_MODEL_PATH,\n",
    "    ANCHORS_PATH,\n",
    "    box_shift=0.2,\n",
    "    box_enlarge=1.3\n",
    ")\n",
    "\n",
    "for name_dir  in os.listdir(PATH):\n",
    "    print(name_dir)\n",
    "    output_file.write(str(name_dir) + \"\\t\" + str(len(os.listdir(PATH + name_dir))) + \"\\t\")\n",
    "    for name_file in os.listdir(PATH + name_dir):\n",
    "        image = cv2.imread(PATH + name_dir + \"/\" + name_file, flags=cv2.IMREAD_COLOR)\n",
    "        points, bbox = detector(image)\n",
    "        \n",
    "        if points is not None:\n",
    "            for p in points:\n",
    "                output_file.write(str(p[0]) + \" \" + str(p[1]) + \" \")\n",
    "        \n",
    "    output_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_SIGN = \"Stop Sign\"\n",
    "SWIPING_LEFT = \"Swiping Left\"\n",
    "SWIPING_RIGHT = \"Swiping Right\"\n",
    "SWIPING_DOWN = \"Swiping Down\"\n",
    "SWIPING_UP = \"Swiping Up\"\n",
    "THUMB_UP = \"Thumb Up\"\n",
    "NO_GESTURE = \"No gesture\"\n",
    "DOING_OTHER_THINGS = \"Doing other things\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"labeles/jester-v1-train.csv\", sep=\";\", header=None)\n",
    "#test_df = pd.DataFrame(\"labeles/jester-v1-test.csv\", sep=\"\\t\", header=None)\n",
    "val_df = pd.read_csv(\"labeles/jester-v1-validation.csv\", sep=\";\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = train_df[train_df[1] == STOP_SIGN]\n",
    "out_df = pd.concat([out_df, val_df[val_df[1] == STOP_SIGN]])\n",
    "\n",
    "out_df = pd.concat([out_df, train_df[train_df[1] == SWIPING_LEFT]])\n",
    "out_df = pd.concat([out_df, val_df[val_df[1] == SWIPING_LEFT]])\n",
    "\n",
    "out_df = pd.concat([out_df, train_df[train_df[1] == SWIPING_RIGHT]])\n",
    "out_df = pd.concat([out_df, val_df[val_df[1] == SWIPING_RIGHT]])\n",
    "\n",
    "out_df = pd.concat([out_df, train_df[train_df[1] == SWIPING_DOWN]])\n",
    "out_df = pd.concat([out_df, val_df[val_df[1] == SWIPING_DOWN]])\n",
    "\n",
    "out_df = pd.concat([out_df, train_df[train_df[1] == SWIPING_UP]])\n",
    "out_df = pd.concat([out_df, val_df[val_df[1] == SWIPING_UP]])\n",
    "\n",
    "out_df = pd.concat([out_df, train_df[train_df[1] == THUMB_UP]])\n",
    "out_df = pd.concat([out_df, val_df[val_df[1] == THUMB_UP]])\n",
    "\n",
    "out_df = pd.concat([out_df, train_df[train_df[1] == NO_GESTURE]])\n",
    "out_df = pd.concat([out_df, val_df[val_df[1] == NO_GESTURE]])\n",
    "\n",
    "out_df = out_df.sample(frac=1).reset_index(drop=True)\n",
    "out_df.to_csv(\"labeles.csv\", index=False, sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33329, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
